{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import azure.cognitiveservices.speech as speechsdk\n",
    "from openai import AzureOpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "AZURE_OPENAI_ENDPOINT = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "AZURE_OPENAI_API_VERSION=\"2024-02-01\"\n",
    "AZURE_OPENAI_API_KEY = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "AZURE_OPENAI_GPT4_DEPLOYMENT_NAME = os.getenv(\"AZURE_OPENAI_GPT4_DEPLOYMENT_NAME\")\n",
    "\n",
    "#init the openai client\n",
    "client = AzureOpenAI(\n",
    "  azure_endpoint = AZURE_OPENAI_ENDPOINT, \n",
    "  api_key=AZURE_OPENAI_API_KEY,  \n",
    "  api_version=\"2023-05-15\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPEECH_KEY = os.getenv(\"SPEECH_KEY\")\n",
    "SPEECH_REGION = os.getenv(\"SPEECH_REGION\")\n",
    "engine_name = \"test\"\n",
    "\n",
    "speech_config = speechsdk.SpeechConfig(subscription=SPEECH_KEY, region=SPEECH_REGION)\n",
    "# Set up Azure Text-to-Speech language \n",
    "speech_config.speech_synthesis_language = \"en-US\"\n",
    "# Set up Azure Speech-to-Text language recognition\n",
    "speech_config.speech_recognition_language = \"en-US\"\n",
    "speech_config.set_property(speechsdk.PropertyId.Speech_LogFilename, \"./log/log.txt\")\n",
    "\n",
    "# Set up the voice configuration\n",
    "speech_config.speech_synthesis_voice_name = \"en-US-JennyMultilingualNeural\"\n",
    "speech_synthesizer = speechsdk.SpeechSynthesizer(speech_config=speech_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def speech_to_text():\n",
    "    # Set up the audio configuration\n",
    "    audio_config = speechsdk.audio.AudioConfig(use_default_microphone=True)\n",
    "\n",
    "    # Create a speech recognizer and start the recognition\n",
    "    speech_recognizer = speechsdk.SpeechRecognizer(speech_config=speech_config, audio_config=audio_config)\n",
    "    print(\"Say something...\")\n",
    "\n",
    "    result = speech_recognizer.recognize_once_async().get()\n",
    "\n",
    "    if result.reason == speechsdk.ResultReason.RecognizedSpeech:\n",
    "        return result.text\n",
    "    elif result.reason == speechsdk.ResultReason.NoMatch:\n",
    "        return \"Sorry, I didn't catch that.\"\n",
    "    elif result.reason == speechsdk.ResultReason.Canceled:\n",
    "        return \"Recognition canceled.\"\n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def speech_from_file(filename):\n",
    "    audio_config = speechsdk.audio.AudioConfig(filename=filename)\n",
    "    # Creates a speech recognizer using a file as audio input, also specify the speech language\n",
    "    speech_recognizer = speechsdk.SpeechRecognizer(\n",
    "        speech_config=speech_config, language=\"en-US\", audio_config=audio_config)\n",
    "\n",
    "    # Starts speech recognition, and returns after a single utterance is recognized. The end of a\n",
    "    # single utterance is determined by listening for silence at the end or until a maximum of about 30\n",
    "    # seconds of audio is processed. It returns the recognition text as result.\n",
    "    # Note: Since recognize_once() returns only a single utterance, it is suitable only for single\n",
    "    # shot recognition like command or query.\n",
    "    # For long-running multi-utterance recognition, use start_continuous_recognition() instead.\n",
    "    result = speech_recognizer.recognize_once()\n",
    "\n",
    "    if result.reason == speechsdk.ResultReason.RecognizedSpeech:\n",
    "        return result.text\n",
    "    elif result.reason == speechsdk.ResultReason.NoMatch:\n",
    "        print(\"No speech could be recognized: {}\".format(result.no_match_details))\n",
    "    elif result.reason == speechsdk.ResultReason.Canceled:\n",
    "        cancellation_details = result.cancellation_details\n",
    "        print(\"Speech Recognition canceled: {}\".format(cancellation_details.reason))\n",
    "        if cancellation_details.reason == speechsdk.CancellationReason.Error:\n",
    "            print(\"Error details: {}\".format(cancellation_details.error_details))\n",
    "    return \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the text-to-speech function\n",
    "def text_to_speech(text):\n",
    "    try:\n",
    "        result = speech_synthesizer.speak_text_async(text).get()\n",
    "        if result.reason == speechsdk.ResultReason.SynthesizingAudioCompleted:\n",
    "            print(\"Text-to-speech conversion successful.\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"Error synthesizing audio: {result}\")\n",
    "            return False\n",
    "    except Exception as ex:\n",
    "        print(f\"Error synthesizing audio: {ex}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_sentiment(text):\n",
    "    system_message = \"\"\"\n",
    "    You are an AI assistant that helps recognize the sentiment in a given text.\n",
    "    1. Evaluate the given text and provide the category of the sentiment as either positive, negative, or neutral.\n",
    "    2. Do not provide any additional examples to the output, just the category.\n",
    "    \"\"\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=AZURE_OPENAI_GPT4_DEPLOYMENT_NAME,\n",
    "        messages = [\n",
    "            {\"role\":\"system\",\"content\":system_message},\n",
    "            {\"role\":\"user\",\"content\":text}\n",
    "            ],\n",
    "        temperature=0   \n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(text, target_language):\n",
    "    system_message = \"\"\"You are a helpful assistant that translates text into \"\"\" + target_language + \"\"\".\n",
    "    Answer in a clear and concise manner only translating the text.\n",
    "    Text:\n",
    "    \"\"\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=AZURE_OPENAI_GPT4_DEPLOYMENT_NAME,\n",
    "        messages = [\n",
    "            {\"role\":\"system\",\"content\":system_message},\n",
    "            {\"role\":\"user\",\"content\":text}\n",
    "            ],\n",
    "        temperature=0   \n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription: Hello, thank you for calling Contoso, who am I speaking with today? Hi, my name is Mary Rondo. I'm trying to enroll myself with Contuso. Hi Mary. Uh, are you calling because you need health insurance? Yes, Yeah, I'm calling to sign up for insurance. Great. Uh, if you can answer a few questions, uh, we can get you signed up in a jiffy. OK. Umm, So, uh, what's your full name?\n"
     ]
    }
   ],
   "source": [
    "source = \"./data/Call1_separated_16k_health_insurance.wav\"\n",
    "text = speech_from_file(source)\n",
    "print(f\"Transcription: {text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment: Neutral\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the sentiment using OpenAI\n",
    "response = evaluate_sentiment(text)\n",
    "print(f\"Sentiment: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translated to Spanish: Hola, gracias por llamar a Contoso, ¿con quién tengo el gusto de hablar hoy? Hola, mi nombre es Mary Rondo. Estoy intentando inscribirme en Contuso. Hola Mary. Eh, ¿llamas porque necesitas seguro de salud? Sí, sí, estoy llamando para inscribirme en un seguro. Genial. Eh, si puedes responder algunas preguntas, eh, podemos inscribirte en un momento. OK. Eh, entonces, ¿cuál es tu nombre completo?\n"
     ]
    }
   ],
   "source": [
    "# Translate the text to Spanish using OpenAI\n",
    "translated_text = translate(text, \"Spanish\")\n",
    "print(f\"Translated to Spanish: {translated_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text-to-speech conversion successful.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the response to speech using text-to-speech\n",
    "text_to_speech(translated_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
